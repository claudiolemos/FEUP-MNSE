# HumanSynth

HumanSynth is an interactive multimedia application that has the goal of generating video and sound using the human body. With the help of a camera and a projector, the user will be able to interact with the application, seeing the result of their actions in real time.

## Members

* Cláudio Fischer Lemos
* Henrique Reis Sendim Rodrigues
* Sandro Miguel Tavares Campos

## Overview

### Sound

The sound in our application will be generated by frequency modulation synthesis. The dependable variables in this will be the base frequency, modulation frequency, amount of modulation and amplitude. With the help of body tracking, we can use the movement of the user’s body parts (hands, legs, head, ...) to create unusual and unexpected sounds.

### Video

As for the video, we will be using segmentation to separate the body from the background, which will be replaced by images and animations, like for example, the format of the sound wave of the sound that is at the moment being generated.

### Technologies

In order to create this application, we will be using machine learning models that will be executed in real-time, on the browser. To facilitate and speed up the development process, we will use ml5js that allows us to integrate BodyPix (for body segmentation) and PoseNet (for body tracking) from the TensorFlow framework. At the same time, the p5.js Javascript library will be use to generate sound.

## Applications and use cases

One of the main use cases for this application will be its installation in galleries or museums focused in audiovisual art and technology. It could also be reworked into having a focus on education.
